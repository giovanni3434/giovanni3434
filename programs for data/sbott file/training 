# Import necessary libraries
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
import json

# Load the training data
questions = []
answers = []

with open("qa_data_1.json", "r") as f:
  training_data = json.load(f)

for q, a in training_data:
  questions.append(q)
  answers.append(a)

# Preprocess the data
vocab_size = 10000  # Keep the top 10,000 words based on word frequency
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(questions + answers)

question_sequences = tokenizer.texts_to_sequences(questions)
answer_sequences = tokenizer.texts_to_sequences(answers)

# Pad the sequences to the same length
max_length = max(len(q) for q in question_sequences)
question_sequences = tf.keras.preprocessing.sequence.pad_sequences(question_sequences, maxlen=max_length, padding="post")
answer_sequences = tf.keras.preprocessing.sequence.pad_sequences(answer_sequences, maxlen=max_length, padding="post")

# Split the data into a training set and a validation set
question_sequences_train, question_sequences_val, answer_sequences_train, answer_sequences_val = train_test_split(question_sequences, answer_sequences, test_size=0.2)

# Build the model
model = tf.keras.Sequential()

# Add the embedding layer
embedding_size = 128  # Set the embedding size to 128
model.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_length))

# Add the LSTM layer
lstm_units = 128  # Set the number of units in the LSTM layer to 128
model.add(tf.keras.layers.LSTM(units=lstm_units))

# Add the output layer
model.add(tf.keras.layers.Dense(units=vocab_size, activation="softmax"))

# Compile the model
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])

# Train the model on the training data
model.fit(question_sequences_train, answer_sequences_train, epochs=10)

# Evaluate the model on the validation data
val_loss, val_acc = model.evaluate(question_sequences_val, answer_sequences_val)

print(f"Validation loss: {val_loss:.4f}, Validation accuracy: {val_acc:.4f}")

# Save the model
model.save("chatbot_model.h5")

#let's test this MF

# Load the saved model
model = tf.keras.models.load_model("chatbot_model.h5")

# Define a question to ask the chatbot
question = "How does a home solar system work?"

# Preprocess the question
question_sequence = tokenizer.texts_to_sequences([question])
question_sequence = tf.keras.preprocessing.sequence.pad_sequences(question_sequence, maxlen=max_length, padding="post")

# Get the chatbot's response
response = model.predict(question_sequence)

# Get the index of the word with the highest probability in the response
response_index = np.argmax(response)

# Get the word corresponding to the index
response_word = tokenizer.index_word[response_index]

print(f"Chatbot: {response_word}")

























